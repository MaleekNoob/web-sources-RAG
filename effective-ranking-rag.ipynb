{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your API key and Programmable Search Engine ID\n",
    "GOOGLE_API_KEY = os.environ.get('GOOGLE_CUSTOM_SEARCH_KEY')\n",
    "SEARCH_ENGINE_ID = os.environ.get('CUSTOM_SEARCH_ENGINE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Generative AI configured AIzaSyAh-0svU7sm77U2238yzfCYvpHFjUre1P8\n"
     ]
    }
   ],
   "source": [
    "# Configure the Google Generative AI\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv('GOOGLE_GEMINI_API_KEY'))\n",
    "\n",
    "print(\"Google Generative AI configured\", os.environ.get('GOOGLE_GEMINI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fetch top search results from Google API\n",
    "def google_search(query, num_results=10):\n",
    "    search_url = f\"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"key\": GOOGLE_API_KEY,\n",
    "        \"cx\": SEARCH_ENGINE_ID,\n",
    "        \"q\": query,\n",
    "        \"num\": num_results,\n",
    "    }\n",
    "    response = requests.get(search_url, params=params)\n",
    "    results = response.json().get(\"items\", [])\n",
    "    return [(r[\"title\"], r[\"snippet\"], r[\"link\"]) for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract and clean web content (basic)\n",
    "def extract_web_content(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers, timeout=5)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        text = \" \".join([p.get_text() for p in paragraphs])\n",
    "        return text[:2000]  # Limit to avoid token overflow\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Summarize using an LLM (Extractive)\n",
    "def summarize_text(text):\n",
    "    prompt = f\"Summarize the following text while keeping key details:\\n\\n{text}\"\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. BM25 Ranking for relevance\n",
    "def bm25_rank(query, documents):\n",
    "    tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    scores = bm25.get_scores(query.lower().split())\n",
    "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    return [documents[i] for i in ranked_indices[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 5. LLM-based relevance filtering (Fixed for Gemini)\n",
    "def rank_relevance(query, snippets):\n",
    "    scored_snippets = []\n",
    "    for snippet in snippets:\n",
    "        prompt = f\"On a scale of 1-10, how relevant is this snippet to '{query}'? Reply with ONLY the number:\\n\\n{snippet}\"\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            raw_score = response.text.strip()\n",
    "\n",
    "            # Extract number using regex\n",
    "            match = re.search(r'\\b([1-9]|10)\\b', raw_score)\n",
    "            if match:\n",
    "                score = int(match.group(1))\n",
    "                scored_snippets.append((snippet, score))\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Skipping invalid response: {raw_score}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            scored_snippets.append((snippet, 0))  # Assign a score of 0 if there's an error\n",
    "\n",
    "    return sorted(scored_snippets, key=lambda x: x[1], reverse=True)[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Embeddings-based re-ranking\n",
    "def embed_and_rank(query, snippets):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    snippet_embeddings = embedding_model.encode(snippets, convert_to_tensor=True)\n",
    "    similarities = util.pytorch_cos_sim(query_embedding, snippet_embeddings)[0]\n",
    "    ranked_indices = similarities.argsort(descending=True)\n",
    "    return [snippets[i] for i in ranked_indices[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Generate a final response using an LLM\n",
    "def generate_final_response(query, snippets):\n",
    "    context = \"\\n\\n\".join(snippets)\n",
    "    prompt = f\"Answer the question '{query}' using the following information:\\n\\n{context}\"\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **üîπ Full Execution Pipeline**\n",
    "def fetch_and_generate_response(user_query):\n",
    "    print(\"üîç Fetching search results...\")\n",
    "    search_results = google_search(user_query, num_results=10)\n",
    "\n",
    "    print(\"üìñ Extracting web content...\")\n",
    "    documents = [extract_web_content(url) for _, _, url in search_results]\n",
    "\n",
    "    print(\"üìù Summarizing extracted content...\")\n",
    "    summaries = [summarize_text(doc) for doc in documents if doc]\n",
    "\n",
    "    print(\"‚ö° Ranking results with BM25...\")\n",
    "    top_summaries = bm25_rank(user_query, summaries)\n",
    "\n",
    "    print(\"üîé Filtering with LLM relevance scoring...\")\n",
    "    top_relevant_summaries = rank_relevance(user_query, top_summaries)\n",
    "\n",
    "    print(\"üìä Embedding and re-ranking...\")\n",
    "    final_snippets = embed_and_rank(user_query, [s[0] for s in top_relevant_summaries])\n",
    "\n",
    "    print(\"ü§ñ Generating final response...\")\n",
    "    return generate_final_response(user_query, final_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Example Usage**\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"How does quantum computing impact cryptography?\"\n",
    "    response = fetch_and_generate_response(user_query)\n",
    "    print(\"\\nüöÄ Final AI Response:\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
